{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与模型开始对话，输入 'exit' 退出。\n",
      "模型说: hi得啚通新赫法实不选整康成出为稓都将襭带\n",
      "模型说: 你为什么不说话诠期网中应而学校有有师你计言就的人是化者以识的女\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# 启动聊天\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[43mchat_with_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 182\u001b[0m, in \u001b[0;36mchat_with_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([tokens], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# 转换为 tensor 并确保它是二维的 (batch_size, seq_len)\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# 使用模型生成新的 tokens\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# 解码生成的 token IDs\u001b[39;00m\n\u001b[0;32m    185\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m safe_decode(generated_tokens[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[1;32mIn[20], line 127\u001b[0m, in \u001b[0;36mGPT.generate\u001b[1;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[0;32m    124\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size \u001b[38;5;28;01melse\u001b[39;00m idx[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mblock_size:]\n\u001b[0;32m    125\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(idx_cond)\n\u001b[1;32m--> 127\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get the last token prediction\u001b[39;00m\n\u001b[0;32m    128\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Apply softmax to get probabilities\u001b[39;00m\n\u001b[0;32m    129\u001b[0m idx_next \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Sample the next token\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import tiktoken  # 用于解码 token\n",
    "import re\n",
    "\n",
    "# GPTConfig 和 GPT 模型架构（和训练时一致）\n",
    "class GPTConfig:\n",
    "    def __init__(self, block_size=512, batch_size=12, n_layer=6, n_head=12, n_embd=768, dropout=0.1, vocab_size=50257):\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.head_size = n_embd // n_head\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'attention_mask', \n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.size()\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        q = self.query(x)\n",
    "        weight = q @ k.transpose(-2, -1)\n",
    "        weight = weight.masked_fill(self.attention_mask[:seq_len, :seq_len] == 0, float('-inf')) / math.sqrt(hidden_size)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(config) for _ in range(config.n_head)])\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        head_size = config.n_embd // config.n_head\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config  # Store config to access block_size\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch, seq_len = idx.size()\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(seq_len, device=idx.device))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            batch, seq_len, vocab_size = logits.size()\n",
    "            logits = logits.view(batch * seq_len, vocab_size)\n",
    "            targets = targets.view(batch * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            logits = logits[:, -1, :]  # Get the last token prediction\n",
    "            probs = F.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample the next token\n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "        return idx\n",
    "\n",
    "# 加载模型\n",
    "def load_model(checkpoint_path='C:/Users/98705/Desktop/LLMs-Zero-to-Hero-master/LLMs-Zero-to-Hero-master/src/video/checkpoints/model_epoch_1.pt', config=None):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"The model checkpoint file was not found at {checkpoint_path}\")\n",
    "    model = GPT(config)  # 使用训练时的配置\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 安全解码函数：去除无效字符\n",
    "def safe_decode(tokens):\n",
    "    try:\n",
    "        # 尝试解码 token 为文本\n",
    "        decoded = encoder.decode(tokens)\n",
    "        # 去除掉可能的乱码字符\n",
    "        decoded = re.sub(r'[^\\w\\s\\u4e00-\\u9fa5]', '', decoded)  # 保留中文字符和常规字母\n",
    "        return decoded\n",
    "    except Exception as e:\n",
    "        print(f\"解码错误: {e}\")\n",
    "        return \"无法解码的文本\"\n",
    "\n",
    "# 交互式聊天函数\n",
    "def chat_with_model():\n",
    "    # 定义训练时的配置\n",
    "    config = GPTConfig()\n",
    "    \n",
    "    # 加载模型\n",
    "    model = load_model('C:/Users/98705/Desktop/LLMs-Zero-to-Hero-master/LLMs-Zero-to-Hero-master/src/video/checkpoints/model_epoch_1.pt', config)\n",
    "\n",
    "    # 初始化tokenizer\n",
    "    encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    print(\"与模型开始对话，输入 'exit' 退出。\")\n",
    "\n",
    "    while True:\n",
    "        # 获取用户输入\n",
    "        user_input = input(\"你说: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"退出对话。\")\n",
    "            break\n",
    "\n",
    "        # 将用户输入转换为 token IDs\n",
    "        tokens = encoder.encode(user_input)  # 将用户输入的文本转换为 tokens\n",
    "        idx = torch.tensor([tokens], dtype=torch.long)  # 转换为 tensor 并确保它是二维的 (batch_size, seq_len)\n",
    "\n",
    "        # 使用模型生成新的 tokens\n",
    "        generated_tokens = model.generate(idx, max_new_tokens=50)\n",
    "\n",
    "        # 解码生成的 token IDs\n",
    "        decoded_text = safe_decode(generated_tokens[0].tolist())\n",
    "        print(f\"模型说: {decoded_text}\")\n",
    "\n",
    "# 启动聊天\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_model()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
